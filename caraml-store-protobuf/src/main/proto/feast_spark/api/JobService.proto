syntax = "proto3";
package feast_spark.api;

option go_package = "github.com/caraml-dev/caraml-store/caraml-store-sdk/go/protos/feast_spark/api";
option java_outer_classname = "JobServiceProto";
option java_package = "dev.caraml.store.protobuf.jobservice";

import "feast/core/DataSource.proto";
import "google/protobuf/timestamp.proto";


service JobService {
    // Start job to ingest data from offline store into online store
    rpc StartOfflineToOnlineIngestionJob (StartOfflineToOnlineIngestionJobRequest) returns (StartOfflineToOnlineIngestionJobResponse);

    // Start job to ingest data from streaming source into online store
    rpc StartStreamIngestionJob(StartStreamIngestionJobRequest) returns (StartStreamIngestionJobResponse);

    // Start scheduled job to ingest data from offline store into online store
    rpc ScheduleOfflineToOnlineIngestionJob (ScheduleOfflineToOnlineIngestionJobRequest) returns (ScheduleOfflineToOnlineIngestionJobResponse);

    // Unschedule job to ingest data from offline store into online store
    rpc UnscheduleOfflineToOnlineIngestionJob(UnscheduleOfflineToOnlineIngestionJobRequest) returns (UnscheduleOfflineToOnlineIngestionJobResponse);

    // Produce a training dataset, return a job id that will provide a file reference
    rpc GetHistoricalFeatures (GetHistoricalFeaturesRequest) returns (GetHistoricalFeaturesResponse);

    // List all types of jobs
    rpc ListJobs (ListJobsRequest) returns (ListJobsResponse);

    // List all scheduled jobs
    rpc ListScheduledJobs (ListScheduledJobsRequest) returns (ListScheduledJobsResponse);

    // Cancel a single job
    rpc CancelJob (CancelJobRequest) returns (CancelJobResponse);

    // Unschedule a job
    rpc UnscheduleJob (UnscheduleJobRequest) returns (UnscheduleJobResponse);

    // Get details of a single job
    rpc GetJob (GetJobRequest) returns (GetJobResponse);

    // Get ingestion health metrics for a Feature Table
    rpc GetHealthMetrics (GetHealthMetricsRequest) returns (GetHealthMetricsResponse);

    // List batch ingestion job record for a Feature Table
    rpc ListBatchJobRecords(ListBatchJobRecordsRequest) returns (ListBatchJobRecordsResponse);
}


enum JobType {
	INVALID_JOB = 0;
	BATCH_INGESTION_JOB = 1;
	STREAM_INGESTION_JOB = 2;
	RETRIEVAL_JOB = 4;
}

enum JobStatus {
	JOB_STATUS_INVALID = 0;
       // The Job has be registered and waiting to get scheduled to run
	JOB_STATUS_PENDING = 1;
       // The Job is currently processing its task
	JOB_STATUS_RUNNING = 2;
       // The Job has successfully completed its task
	JOB_STATUS_DONE = 3;
       // The Job has encountered an error while processing its task
	JOB_STATUS_ERROR = 4;
}

message ScheduledJob {
  // Identifier of the Job
  string id = 1;
  string table_name = 2;
  string project = 3;
  // Timespan of the ingested data per job, in days. The data from  end of the day - timespan till end of the day will be ingested. Eg. if the job execution date is 10/4/2021, and ingestion timespan is 2, then data from 9/4/2021 00:00 to 10/4/2021 23:59 (inclusive) will be ingested.
  int32 ingestion_timespan = 4;
  // Crontab string. Eg. 0 13 * * *
  string cron_schedule = 5;
}

message Job {
  // Identifier of the Job
  string id = 1;
  // Type of the Job
  JobType type = 2;
  // Current job status
  JobStatus status = 3;
  // Deterministic hash of the Job
  string hash = 4;
  // Start time of the Job
  google.protobuf.Timestamp start_time = 5;

  message RetrievalJobMeta {
    string output_location = 1;
  }

  message OfflineToOnlineMeta {
      string table_name = 1;
  }

  message StreamToOnlineMeta {
      string table_name = 1;
  }

  // JobType specific metadata on the job
  oneof meta {
    RetrievalJobMeta retrieval = 6;
    OfflineToOnlineMeta batch_ingestion = 7;
    StreamToOnlineMeta stream_ingestion = 8;
  }

  // Path to Spark job logs, if available
  string log_uri = 9;

  // Spark job error message, if available
  string error_message = 10;

  // Project
  string project = 11;
}

message BatchJobRecord {
  string job_id = 1;
  // Type of the Job
  JobType type = 2;
  // Current job status
  JobStatus status = 3;
  google.protobuf.Timestamp job_start_time = 4;
  google.protobuf.Timestamp job_end_time = 5;
  message OfflineToOnlineMeta {
    string table_name = 1;
    string project = 2;
    google.protobuf.Timestamp start_time = 3;
    google.protobuf.Timestamp end_time = 4;
  }
  // JobType specific metadata on the job
  oneof meta {
    OfflineToOnlineMeta batch_ingestion = 6;
    // TODO: add RetrievalJobMeta
  }
  string spark_app_manifest = 7;
}

// Ingest data from offline store into online store
message StartOfflineToOnlineIngestionJobRequest {
    // Feature table to ingest
    string project = 1;
    string table_name = 2;

    // Start of time range for source data from offline store
    google.protobuf.Timestamp start_date = 3;

    // End of time range for source data from offline store
    google.protobuf.Timestamp end_date = 4;

    // optional setting for delta ingestion
    bool delta_ingestion = 5;
}

message StartOfflineToOnlineIngestionJobResponse {
  // Job ID assigned by Feast
  string id = 1;

  // Job start time
  google.protobuf.Timestamp job_start_time = 2;

  // Feature table associated with the job
  string table_name = 3;

  // Path to Spark job logs, if available
  string log_uri = 4;
}

// Ingest data from streaming source into online store
message StartStreamIngestionJobRequest {
  // Feature table to ingest
  string project = 1;
  string table_name = 2;
}

message StartStreamIngestionJobResponse {
  // Job ID assigned by Feast
  string id = 1;
}

message ScheduleOfflineToOnlineIngestionJobRequest {
    // Feature table to ingest
    string project = 1;
    string table_name = 2;


    // Timespan of the ingested data per job, in days. The data from  end of the day - timespan till end of the day will be ingested. Eg. if the job execution date is 10/4/2021, and ingestion timespan is 2, then data from 9/4/2021 00:00 to 10/4/2021 23:59 (inclusive) will be ingested.
    int32 ingestion_timespan = 3;

   // Crontab string. Eg. 0 13 * * *
    string cron_schedule = 4;

}

message ScheduleOfflineToOnlineIngestionJobResponse {}

message UnscheduleOfflineToOnlineIngestionJobRequest {
  string project = 1;
  string table_name = 2;
}

message UnscheduleOfflineToOnlineIngestionJobResponse {}

message GetHistoricalFeaturesRequest {
  // List of feature references that are being retrieved
  repeated string feature_refs = 1;

  // Batch DataSource that can be used to obtain entity values for historical retrieval.
  // For each entity value, a feature value will be retrieved for that value/timestamp
  // Only 'BATCH_*' source types are supported.
  // Currently only BATCH_FILE source type is supported.
  feast.core.DataSource entity_source = 2;

  // Optional field to specify project name override. If specified, uses the
  // given project for retrieval. Overrides the projects specified in
  // Feature References if both are specified.
  string project = 3;

  // Specifies the path in a bucket to write the exported feature data files
  // Export to AWS S3 - s3://path/to/features
  // Export to GCP GCS -  gs://path/to/features
  string output_location = 4;

  // Specify format name for output, eg. parquet
  string output_format = 5;
}

message GetHistoricalFeaturesResponse {
  // Export Job with ID assigned by Feast
  string id = 1;

  // Uri to the join result output file
  string output_file_uri = 2;

  // Job start time
  google.protobuf.Timestamp job_start_time = 3;

  // Path to Spark job logs, if available
  string log_uri = 4;

}

message ListJobsRequest {
  bool include_terminated = 1;
  string table_name = 2;
  string project = 3;
  JobType type = 4;
}

message ListScheduledJobsRequest {
  string project = 1;
  string table_name = 2;
}

message ListJobsResponse {
  repeated Job jobs = 1;
}

message ListScheduledJobsResponse {
  repeated ScheduledJob jobs = 1;
}

message GetJobRequest {
  string job_id = 1;
}

message GetJobResponse {
  Job job = 1;
}

message CancelJobRequest{
  string job_id = 1;
}

message CancelJobResponse {}

message UnscheduleJobRequest {
  string job_id = 1;
}

message UnscheduleJobResponse {}

message GetHealthMetricsRequest {
  string project = 1;
  repeated string table_names = 2;
}

message GetHealthMetricsResponse {
  repeated string passed = 1;
  repeated string failed = 2;
}

message ListBatchJobRecordsRequest {
  // Project name
  string project = 1;

  // Feature table name, required for JOB_TYPE=BATCH_INGESTION_JOB
  string table_name = 2;

  // optional field
  JobType type = 3;

  // Optional, Time range for the records to be listed. Defaults to preconfigured range
  google.protobuf.Timestamp from = 4;

  // Optional, defaults to current time
  google.protobuf.Timestamp to = 5;
}

message ListBatchJobRecordsResponse {
  repeated BatchJobRecord records = 1;
}