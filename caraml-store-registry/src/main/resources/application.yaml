caraml:
  registry:
    # Whether to create/update streaming ingestion jobs on feature table creation/update
    syncIngestionJobOnSpecUpdate: true
    featureTableDefaultMaxAgeSeconds: 604800 # 7 days

  # Enable integration with MLP server
  mlp:
    enabled: true
    client:
      endpoint: "http://localhost:9090/api"
      connectionTimeOutMs: 3000
      requestTimeOutMs: 1500
      authEnabled: false
    cache:
      # Initial delay before scheduling the cache refresh (in milliseconds)
      initialDelayMs: 60000
      # Frequency of refreshing cache (in milliseconds)
      refreshIntervalMs: 60000
    context:
      # Fallback team/stream to use in the event the MLP server is unable to return a response
      fallbackTeam: "unknown"
      fallbackStream: "unknown"

  resilience4j:
    retry:
      instances:
        listProjects:
          maxRetryAttempts: 3
          waitDuration: 500ms

  kubernetes:
    # Whether to use in-cluster Kubernetes configuration or default kube config.
    inCluster: false

  monitoring:
    # Configure the behaviour of the timer metrics
    timer:
      # Published percentiles
      percentiles:
        - 0.5
        - 0.95
        - 0.99
      # Minimum expected latency
      minBucketMs: 1
      # Maximum expected latency
      maxBucketMs: 200

  # SparkApplicationSpec for ingestion and historical retrieval jobs. Refer to the official Spark Operator documentation
  # for the full range of options supported. Feature schema and source information will be appended to existing arguments
  # as the flag arguments --feature-table and --store. For batch ingestion jobs, --start-time and --end-time will be added
  # to indicate the range of data to be ingested.
  jobService:
    namespace: spark-operator
    common:
      sparkImage: "ghcr.io/caraml-dev/caraml-store-spark:latest"

    streamIngestion:
      - store: default-store
        # Stream ingestion spark application spec
        # sparkApplicationSpec:
    batchIngestion:
      - store: default-store
        # Batch ingestion spark application spec
        # sparkApplicationSpec:
    historicalRetrieval:
      # Historical retrieval job spark application spec
       sparkApplicationSpec:
         driver:
           cores: 1
           memory: "2048m"
         executor:
           cores: 1
           instances: 1
           memory: "6144m"
    # Default store for ingestion if store name is not specified in the feature table
    defaultStore:
      stream: default-store
      batch: default-store
    # BigQuery project where temp tables are stored for delta ingestion
    deltaIngestionDataset:
      project: bq-project
      dataset: bq-dataset

grpc:
  server:
    port: 6565

spring:
  jpa:
    properties.hibernate:
      format_sql: true
      event:
        merge:
          entity_copy_observer: allow
    hibernate.naming.physical-strategy=org.hibernate.boot.model.naming: PhysicalNamingStrategyStandardImpl
    hibernate.ddl-auto: validate
  datasource:
    driverClassName: org.postgresql.Driver
    url: jdbc:postgresql://${DB_HOST:127.0.0.1}:${DB_PORT:5432}/${DB_DATABASE:postgres}
    username: ${DB_USERNAME:postgres}
    password: ${DB_PASSWORD:password}
  flyway:
    baseline-on-migrate: true

management:
  endpoints:
    web:
      exposure:
        include: prometheus,health
    metrics:
      enabled: true