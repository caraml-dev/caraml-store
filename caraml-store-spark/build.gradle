plugins {
    id 'caraml.scala'
    id 'application'
    id "com.github.johnrengelman.shadow" version "7.1.2"
    id 'com.palantir.docker' version "0.34.0"
    id 'com.github.maiflai.scalatest' version "0.32"
    id 'com.google.protobuf'
    id 'idea'
}

ext {
    sparkVersion = '3.3.4'
    scalaVersion = '2.12'
    protobufVersion = '3.19.2'
    hbaseVersion = '2.4.11'
}

java {
    sourceCompatibility = JavaVersion.VERSION_11
    targetCompatibility = JavaVersion.VERSION_11
}


repositories {
    mavenCentral()
}

configurations {
    testImplementation.extendsFrom compileOnly
}

dependencies {
    compileOnly "org.apache.spark:spark-sql_$scalaVersion:$sparkVersion"
    compileOnly "org.apache.spark:spark-catalyst_$scalaVersion:$sparkVersion"
    compileOnly "org.apache.spark:spark-streaming_$scalaVersion:$sparkVersion"
    implementation "org.apache.spark:spark-avro_$scalaVersion:$sparkVersion"
    implementation "org.apache.spark:spark-sql-kafka-0-10_$scalaVersion:${sparkVersion}"
    implementation project(':caraml-store-protobuf')
    implementation 'io.odpf:stencil:0.2.1'
    implementation 'joda-time:joda-time:2.10.6'
    implementation "com.github.scopt:scopt_$scalaVersion:3.7.1"
    implementation 'redis.clients:jedis:4.1.1'
    implementation('com.google.cloud.bigtable:bigtable-hbase-2.x-hadoop:2.14.3') {
        exclude group: "org.apache.hadoop", module: "hadoop-common"
    }
    implementation "org.apache.hbase:hbase-client:$hbaseVersion"
    implementation "org.apache.hbase:hbase-mapreduce:${hbaseVersion}"
    implementation('org.glassfish:javax.el') {
        version {
            strictly '3.0.1-b12'
        }
    }
    implementation "org.json4s:json4s-ext_$scalaVersion:3.7.0-M5"
    implementation 'com.bucket4j:bucket4j-core:8.5.0'
    testImplementation "org.scalatest:scalatest_$scalaVersion:3.2.2"
    testImplementation "org.scalacheck:scalacheck_$scalaVersion:1.14.3"
    testImplementation "com.dimafeng:testcontainers-scala-scalatest_$scalaVersion:0.40.12"
    // Downgraded to the version of wiremock which still use jackson 2.10.0
    testImplementation 'com.github.tomakehurst:wiremock-jre8:2.26.3'
    testImplementation "com.dimafeng:testcontainers-scala-kafka_$scalaVersion:0.40.12"
    testRuntimeOnly 'com.vladsch.flexmark:flexmark-all:0.35.10'
}
application {
    mainClassName = 'dev.caraml.spark.IngestionJob'
}

shadowJar {
    archiveFileName = "caraml-spark-application-with-dependencies.jar"
    project.configurations.implementation.canBeResolved = true
    configurations = [project.configurations.implementation]
    mergeServiceFiles()
    exclude "META-INF/*.SF"
    exclude "META-INF/*.DSA"
    exclude "META-INF/*.RSA"
    relocate "com.google.protobuf", "com.google.protobuf.vendor"
    zip64 true
}

def containerRegistry = System.getenv('DOCKER_REGISTRY')

docker {
    dependsOn shadowJar
    dockerfile file('docker/Dockerfile')
    files shadowJar.outputs, "$rootDir/caraml-store-pyspark/scripts"
    copySpec.with {
        from("$rootDir/caraml-store-pyspark") {
            include 'templates/**'
            into 'templates'
        }
    }
    name "${containerRegistry == null ? "" : containerRegistry + "/"}${project.name}:${version}"
}

test {
    // Required so that Spark can run on Java 17 during test. Not required when using spark-submit.
    doFirst {
        jvmArgs = [
                "--add-opens=java.base/java.lang=ALL-UNNAMED",
                "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED",
                "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED",
                "--add-opens=java.base/java.io=ALL-UNNAMED",
                "--add-opens=java.base/java.net=ALL-UNNAMED",
                "--add-opens=java.base/java.nio=ALL-UNNAMED",
                "--add-opens=java.base/java.util=ALL-UNNAMED",
                "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED",
                "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED",
                "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED",
                "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED",
                "--add-opens=java.base/sun.security.action=ALL-UNNAMED",
                "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED"
        ]
    }

    maxParallelForks = 1
}

protobuf {
    protoc {
        artifact = "com.google.protobuf:protoc:${protobufVersion}"
    }
    generatedFilesBaseDir = "$projectDir/src/generated"
    clean {
        delete generatedFilesBaseDir
    }
    generateProtoTasks {
        all().each { task ->
            task.generateDescriptorSet = true
            task.descriptorSetOptions.path = "$generatedFilesBaseDir/${task.sourceSet.name}/resources/stencil/__files/descriptor.desc"
            task.descriptorSetOptions.includeImports = true
        }

        ofSourceSet('test')
    }
}

idea {
    module {
        testSourceDirs += file("src/generated/test/java")
        testResourceDirs += file("src/generated/test/resources")
        generatedSourceDirs += file("src/generated/test/java")
    }
}

sourceSets {
    test {
        resources {
            srcDirs "src/generated/test/resources"
        }
    }
}
